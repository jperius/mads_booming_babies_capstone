{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57c7b8e6",
   "metadata": {},
   "source": [
    "<h2>Note: Most of the code in this notebook will either run and hang indefinitely, error out, or simply has not been tested!</h2>\n",
    "\n",
    "This notebook was used for testing out a few different libraries, and as a staging area for some of the residualization & interaction term generation methodology we were going to try to utilize. Most of the DAG-generating code will not run, and the functions near the bottom were never tested. This is maintained for record. I may also tap into the redisualization / interaction functions at work to see how feasible it might be to productize a more causally-driven Marketing-Mix Model (MMM)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8026255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from causallearn.search.ConstraintBased.PC import pc\n",
    "from causallearn.search.ScoreBased.GES import ges\n",
    "from causallearn.utils.PDAG2DAG import pdag2dag\n",
    "from dowhy import gcm\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, PoissonRegressor\n",
    "import statsmodels.api as sm\n",
    "from itertools import combinations\n",
    "import networkx as nx\n",
    "import warnings\n",
    "from IPython.display import display\n",
    "\n",
    "BASE_DIR = Path().resolve()\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=FutureWarning,\n",
    "    module=\"causallearn\"\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\".*\", category=FutureWarning, module=\"pandas\")\n",
    "\n",
    "get_ipython().kernel.shell.displayhook.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e182e6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(BASE_DIR / \"natality_7yr_test_data_for_dag.csv\")\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f5a8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled = (\n",
    "    df.groupby(df['date'].dt.year, group_keys=False)\n",
    "      .apply(lambda g: g.sample(n=5000, random_state=42))\n",
    "      .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105d9080",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_cols_to_transform = [\n",
    "    'bmi', 'time_sin', 'time_cos'\n",
    "]\n",
    "\n",
    "sampled[continuous_cols_to_transform].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6febcbe8",
   "metadata": {},
   "source": [
    "Since most of our variables are one-hot / binary, we're going to convert them to bins so we can use one algorithm to generate & explore the DAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d52b83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're dropping these because we have discrete versions of them, which might be more informative for the DAG.\n",
    "binary_cols_to_drop = [\"cig_0_binary\", \"cig_1_binary\", \"cig_2_binary\", \"cig_3_binary\", \"priorlive_binary\", \"priordead_binary\", \"priorterm_binary\", \"precare_binary\"]\n",
    "df_discrete = sampled.drop(columns=binary_cols_to_drop)\n",
    "\n",
    "discretizer = KBinsDiscretizer(\n",
    "    n_bins=4,\n",
    "    encode='ordinal',\n",
    "    strategy='quantile',\n",
    "    quantile_method='averaged_inverted_cdf'\n",
    ")\n",
    "\n",
    "df_discrete[continuous_cols_to_transform] = discretizer.fit_transform(df_discrete[continuous_cols_to_transform])\n",
    "df_discrete[continuous_cols_to_transform] = df_discrete[continuous_cols_to_transform].astype('int32')\n",
    "\n",
    "X = df_discrete.drop(columns=[\"date\"]).astype(int).to_numpy()\n",
    "names = df_discrete.drop(columns=[\"date\"]).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a304f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_df = df_discrete.describe()\n",
    "\n",
    "describe_df.to_csv(\"describe_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762ada5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cg1 = ges(\n",
    "    X,\n",
    "    score_func=\"local_score_BDeu\", # or \"bic\"\n",
    "    node_names=names\n",
    ")\n",
    "\n",
    "with open(\"ges.pkl\", \"wb\") as f:\n",
    "    pickle.dump(cg1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d020f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "cg2 = pc(\n",
    "    X,\n",
    "    indep_test=\"gsq\",\n",
    "    alpha=0.05,\n",
    "    stable=False,\n",
    "    node_names=names\n",
    ")\n",
    "\n",
    "with open(\"pc.pkl\", \"wb\") as f:\n",
    "    pickle.dump(cg2, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e35018b",
   "metadata": {},
   "source": [
    "For either above, must convert from Partially Direct Acyclic Graph to Direct Acyclic Graph for use with DoWhy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1771a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causallearn_to_networkx(adjacency_matrix, node_names):\n",
    "    \"\"\"\n",
    "    Convert causal-learn adjacency matrix to NetworkX DiGraph\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add nodes\n",
    "    for name in node_names:\n",
    "        G.add_node(name)\n",
    "    \n",
    "    # Add edges where adjacency_matrix[i,j] != 0 means i -> j\n",
    "    for i in range(len(adjacency_matrix)):\n",
    "        for j in range(len(adjacency_matrix)):\n",
    "            if adjacency_matrix[i, j] == -1 and adjacency_matrix[j, i] == 1:\n",
    "                # i -> j (tail at i, head at j)\n",
    "                G.add_edge(node_names[i], node_names[j])\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5071db02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PDAG to DAG\n",
    "dag = pdag2dag(cg.G)\n",
    "\n",
    "adjacency_matrix = dag.graph\n",
    "\n",
    "nx_graph = causallearn_to_networkx(adjacency_matrix, names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89707353",
   "metadata": {},
   "source": [
    "pgmpy HillClimbSearch + BDeu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efd18af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.estimators import HillClimbSearch, BDeuScore\n",
    "\n",
    "hc = HillClimbSearch(df, scoring_method=BDeuScore(df, equivalent_sample_size=1))\n",
    "model = hc.estimate()  # returns a DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01931f1f",
   "metadata": {},
   "source": [
    "pgmpy Max-Min Hill-Climb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e29c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.estimators import maxmin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea315d06",
   "metadata": {},
   "source": [
    "<h2>Explore</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a38651",
   "metadata": {},
   "source": [
    "<h2>Clean Causal Relationships and Build Interaction Terms</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7112bf",
   "metadata": {},
   "source": [
    "- Use PCM when you only need interventions/predictions and want maximum flexibility in mechanism choice\n",
    "- Use SCM when you need counterfactual reasoning (\"what if X had been different?\")\n",
    "\n",
    "Since we're using AdditiveNoiseModel, PCM is more or less the same as SCM.\n",
    "\n",
    "AdditiveNoiseModel\n",
    "\n",
    "AdditiveNoiseModel represents a continuous functional causal model of the form Y = f(X) + N, where X is the input (typically the direct causal parents of Y) and the noise N is assumed to be independent of X.\n",
    "\n",
    "Each variable Y is modeled as: Y = (linear combination of parents) + independent noise.\n",
    "\n",
    "When you call gcm.fit(pcm, df), it:\n",
    "\n",
    "1. Fits a linear regression from parents → child for each node\n",
    "2. Computes residuals to learn the noise distribution\n",
    "3. Stores both the regression model and noise model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe6c437",
   "metadata": {},
   "source": [
    "3 main stages:\n",
    "\n",
    "DAG-Informed\n",
    "\n",
    "- Partial out: Partialing-out (residualization) removes confounding and mediation bias, but it only projects away linear additive relationships between each variable and its parents. It can’t invent or preserve interaction structure. Residualization step protects causal identification but not functional completeness.\n",
    "\n",
    "- Add Interactions: captures moderation or how one cause modifies another’s impact on Y. Must be direct parents of Y, not ancestors or descendents. If the don't directly cause Y, they’re outside Y’s equation.\n",
    "\n",
    "Optimization\n",
    "\n",
    "- Feature Selection: MOO for AIC, Precision and Condition Number. Analyze Pareto Front, and choose feature set based on model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292e2676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_binary(s):\n",
    "    vals = pd.Series(s).dropna().unique()\n",
    "    return len(vals) <= 2 and set(vals).issubset({0,1,True,False})\n",
    "\n",
    "def _is_count(s):\n",
    "    s_nonan = pd.Series(s).dropna()\n",
    "    return (s_nonan.dtype.kind in \"iu\") and (s_nonan.min() >= 0)\n",
    "\n",
    "def fit_linear_gcm(G_nx, df):\n",
    "    pcm = gcm.ProbabilisticCausalModel(G_nx)\n",
    "    for v in G_nx.nodes():\n",
    "        y = df[v]\n",
    "        if _is_binary(y):\n",
    "            mech = LogisticRegression(max_iter=1000)\n",
    "        elif _is_count(y):\n",
    "            mech = PoissonRegressor(alpha=0.0, max_iter=1000)\n",
    "        else:\n",
    "            mech = LinearRegression()\n",
    "        pcm.set_causal_mechanism(v, gcm.AdditiveNoiseModel(mech))\n",
    "    gcm.fit(pcm, df)\n",
    "    return pcm\n",
    "\n",
    "def _predict_any(model, X):\n",
    "    # logistic uses expected value (prob of class 1)\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        return model.predict_proba(X)[:, 1]\n",
    "    # linear, etc.\n",
    "    return model.predict(X)\n",
    "\n",
    "def residualize_all_nodes(pcm, G_nx, df, n_splits=5, random_state=42):\n",
    "    df_out = df.copy()\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    for v in G_nx.nodes():\n",
    "        ps = list(G_nx.predecessors(v))\n",
    "        if not ps:\n",
    "            continue\n",
    "\n",
    "        X = df[ps].values\n",
    "        y = df[v].values\n",
    "        preds = np.zeros(len(df))\n",
    "\n",
    "        # clone the already-chosen mechanism type from pcm for consistency\n",
    "        base_model = pcm.causal_mechanism(v).prediction_model\n",
    "\n",
    "        for train_idx, test_idx in kf.split(X):\n",
    "            # fresh clone to avoid leakage across folds\n",
    "            model = type(base_model)(**getattr(base_model, \"get_params\", lambda: {})())\n",
    "            model.fit(X[train_idx], y[train_idx])\n",
    "            preds[test_idx] = _predict_any(model, X[test_idx])\n",
    "\n",
    "        df_out[f\"{v}_resid\"] = y - preds\n",
    "\n",
    "    return df_out\n",
    "\n",
    "def parents(G, v):\n",
    "    return list(G.predecessors(v))\n",
    "\n",
    "def _resid_or_raw(df, col):\n",
    "    return f\"{col}_resid\" if f\"{col}_resid\" in df.columns else col\n",
    "\n",
    "def candidate_interactions_for_Y(G, Y):\n",
    "    \"\"\"\n",
    "    GeneratesDAG-valid interactions, aka unordered pairs among parents of\n",
    "    y that are not ancestor–descendant.\n",
    "    \"\"\"\n",
    "    ps = parents(G, Y)\n",
    "    pairs = []\n",
    "    for a, b in combinations(ps, 2):\n",
    "        if nx.has_path(G, a, b) or nx.has_path(G, b, a):\n",
    "             # skips hierarchical pairs\n",
    "            continue\n",
    "        pairs.append((a, b))\n",
    "    return pairs\n",
    "\n",
    "def add_interactions(df, pairs):\n",
    "    \"\"\"\n",
    "    Mean-center inputs, then create products for each (a,b).\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    # collect the columns we’ll use (residualized if they exist)\n",
    "    cols = sorted({ _resid_or_raw(df2, a) for ab in pairs for a in ab })\n",
    "    # mean-center to stabilize and make main effects interpretable at the mean\n",
    "    centered = df2[cols] - df2[cols].mean(axis=0)\n",
    "    for a, b in pairs:\n",
    "        ca, cb = _resid_or_raw(df2, a), _resid_or_raw(df2, b)\n",
    "        df2[f\"{a}__x__{b}\"] = centered[ca] * centered[cb]\n",
    "    return df2\n",
    "\n",
    "def design_for_Y(G, df_resid, Y):\n",
    "    \"\"\"\n",
    "    Creates the cleaned, augmented design matrix and returns X (with intercept) and y.\n",
    "    \"\"\"\n",
    "    ps = parents(G, Y)\n",
    "    base_cols = [_resid_or_raw(df_resid, p) for p in ps]\n",
    "\n",
    "    # main effects (centered)\n",
    "    X_base = df_resid[base_cols].copy()\n",
    "    X_base = X_base - X_base.mean(axis=0)\n",
    "\n",
    "    # interactions among valid co-parents\n",
    "    pairs = candidate_interactions_for_Y(G, Y)\n",
    "    X_all = add_interactions(pd.concat([X_base], axis=1), pairs)\n",
    "\n",
    "    # assemble final design\n",
    "    covs = list(X_base.columns) + [f\"{a}__x__{b}\" for (a, b) in pairs]\n",
    "    X = sm.add_constant(X_all[covs], has_constant=\"add\")\n",
    "    y = df_resid[Y]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b122859",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcm = fit_linear_gcm(G_nx, df)\n",
    "df_resid = residualize_all_nodes(pcm, G_nx, df)\n",
    "\n",
    "X_tot, y = design_for_Y(G_nx, df_resid, 'no_mmorb')\n",
    "res_tot = sm.OLS(y, X_tot).fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
