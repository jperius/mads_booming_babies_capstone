{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96738e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import r2_score, f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "import optuna\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "BASE_DIR = Path().resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe2d830",
   "metadata": {},
   "source": [
    "<h2>Tidying Up</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c43d008",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(BASE_DIR / \"natality_7yr_test_data.csv\")\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635a29e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96695488",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cols = [col for col in df.columns if df[col].nunique() == 2 and df[col].dtype == 'int64']\n",
    "\n",
    "binary_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9354c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['imp_sex'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0933b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('imp_sex', axis=1)\n",
    "\n",
    "df['date'] = pd.to_datetime(df['dob_yy'].astype(str) + df['dob_wk'].astype(str) + '1', format='%G%V%u')\n",
    "\n",
    "# Fourier terms for time of day as a type of \"seasonality\"\n",
    "df['time_str'] = df['dob_tt'].astype(str).str.zfill(4)\n",
    "df['hour'] = df['time_str'].str[:2].astype(int)\n",
    "df['minute'] = df['time_str'].str[2:].astype(int)\n",
    "\n",
    "df['minute_of_day'] = df['hour'] * 60 + df['minute']\n",
    "df['time_sin'] = np.sin(2 * np.pi * df['minute_of_day'] / 1440)\n",
    "df['time_cos'] = np.cos(2 * np.pi * df['minute_of_day'] / 1440)\n",
    "\n",
    "# Monthly Fourier terms for monthly \"seasonality\"\n",
    "# df['month_sin'] = np.sin(2 * np.pi * df['dob_mm'] / 12)\n",
    "# df['month_cos'] = np.cos(2 * np.pi * df['dob_mm'] / 12)\n",
    "\n",
    "df = df.drop(['dob_yy', 'dob_tt', 'dob_wk', 'time_str', 'minute_of_day', 'minute', 'hour'], axis=1)\n",
    "\n",
    "df['sex'] = np.where(df['sex'] == 'M', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa30d7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['no_mmorb'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31447168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean outcome and drop where we don't have a label (we aren't at risk of data-shortage lol)\n",
    "\n",
    "df = df.rename(columns={'no_mmorb': 'death_reported'})\n",
    "\n",
    "df = df[df['death_reported'] != 9]\n",
    "# flipping the binary so mother's death is the positive class\n",
    "df['death_reported'] = 1 - df['death_reported']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62929136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[col for col in df.columns if col not in binary_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de408ff",
   "metadata": {},
   "source": [
    "List of variables that require some cleaning in their encodings - lots of sentinel values.\n",
    "\n",
    "- fagecomb - 99 is a sentinel value; will need to drop or fill it\n",
    "- priorlive - 99 is a sentinel value\n",
    "- priordead - 99 sentinel value\n",
    "- priorterm - 99\n",
    "- *illb_r (Interval Since Last Live Birth Recode) - 000–003 are sentinel; 888 - Not applicable — first live birth; 999 - Unknown or not stated\n",
    "- ilop_r (Interval Since Last Other Pregnancy Recode) - not sure if we want to use; same sentinel as illb_r\n",
    "- *ilp_r (Interval Since Last Pregnancy Recode) - not sure we want to use; same sentinel as illb_r\n",
    "- ilp_r11 (Interval Since Last Pregnancy Recode 11) - same as above\n",
    "- precare - 99\n",
    "- previs - 99\n",
    "- cig_0 - cig_3 - 99\n",
    "- bmi - 99.9\n",
    "- pwgt_r - 999\n",
    "- dwgt_r - 999\n",
    "- wtgain - 99\n",
    "- rf_cesarn - 99\n",
    "- combgest - 99\n",
    "- dbwt - 999\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef9f4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_99 = [\n",
    "    'fagecomb', 'priorlive', 'priordead', 'priorterm',\n",
    "    'precare', 'previs', 'cig_1', 'cig_2', 'cig_3',\n",
    "    'wtgain', 'rf_cesarn', 'combgest'\n",
    "]\n",
    "df[cols_99] = df[cols_99].replace(99, np.nan)\n",
    "\n",
    "df[['pwgt_r', 'dwgt_r', 'dbwt']] = df[['pwgt_r', 'dwgt_r', 'dbwt']].replace(999, np.nan)\n",
    "\n",
    "df['bmi'] = df['bmi'].replace(99.9, np.nan)\n",
    "df.loc[df['bmi'] > 90, 'bmi'] = np.nan\n",
    "\n",
    "df[['illb_r', 'ilp_r']] = df[['illb_r', 'ilp_r']].replace(\n",
    "    [0, 1, 2, 3, 888, 999],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "df[['ca_down', 'ca_disor']] = df[['ca_down', 'ca_disor']].replace('C', 2).astype(int)\n",
    "\n",
    "# These seem redundant, so removed for now - can always comment out.\n",
    "df = df.drop(columns=['ilop_r', 'ilp_r11'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b694f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_cols = [ # 'illb_r', 'ilp_r' <- dropped later on due to too many missing values\n",
    "    'mager', 'fagecomb', 'priorlive', 'priordead', 'priorterm',\n",
    "    'precare', 'previs', 'cig_0', 'cig_1', 'cig_2',\n",
    "    'cig_3', 'bmi', 'pwgt_r', 'dwgt_r', 'wtgain', 'rf_cesarn', 'combgest',\n",
    "    'dbwt', 'time_sin', 'time_cos' #, 'month_sin', 'month_cos'\n",
    "]\n",
    "cat_cols = [\n",
    "    'bfacil', 'mracehisp', 'mar_p', 'meduc', 'fracehisp', 'feduc',\n",
    "    'cig_rec', 'rf_pdiab', 'rf_gdiab', 'rf_phype', 'rf_ghype', \n",
    "    'rf_ehype', 'rf_ppterm', 'rf_inftr', 'rf_fedrg', 'rf_artec',\n",
    "    'rf_cesar', 'ip_gon', 'ip_syph', 'ip_chlam', 'ip_hepb', 'ip_hepc',\n",
    "    'ld_indl', 'ld_augm', 'ld_ster', 'ld_antb', 'ld_chor', 'ld_anes',\n",
    "    'me_pres', 'me_rout', 'me_trial', 'mm_mtr', 'mm_plac', 'mm_rupt',\n",
    "    'mm_uhyst', 'mm_aicu', 'attend', 'pay', 'dplural',\n",
    "    'ab_aven1', 'ab_aven6', 'ab_nicu', 'ab_surf', 'ab_anti', 'ab_seiz',\n",
    "    'ca_down', 'ca_disor', 'dob_mm' # using monthly seasonality as one-hot instead of Fourier. Will be more descriptive.\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c29883b",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "for col in cat_cols:\n",
    "    col_data = df[col].astype(\"string\")\n",
    "\n",
    "    encoded = enc.fit_transform(col_data.to_frame())\n",
    "    encoded_cols = enc.get_feature_names_out([col])\n",
    "    encoded_df = pd.DataFrame(encoded, columns=encoded_cols, index=df.index)\n",
    "\n",
    "    df = pd.concat([df, encoded_df], axis=1)\n",
    "    df = df.drop(columns=[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0a233f",
   "metadata": {},
   "source": [
    "Variables we're dropping to form the 'baseline' or intercept in Logistic Regression. We don't have to one-hot if we end up using a tree method for anything.\n",
    "\n",
    "- bfacil_1 - Hospital\n",
    "- mracehisp_1 - Non-Hispanic White (only)\n",
    "- mar_p_1 - Yes\n",
    "- meduc_1 - 8th grade or less\n",
    "- fracehisp_1 - Non-Hispanic White (only)\n",
    "- feduc_1 - 8th grade or less\n",
    "- cig_rec_0 - No\n",
    "- rf_pdiab_0 - No\n",
    "- rf_gdiab_0 - No\n",
    "- rf_phype_0 - No\n",
    "- rf_ghype_0 - No\n",
    "- rf_ehype_0 - No\n",
    "- rf_ppterm_0 - No\n",
    "- rf_inftr_0 - No\n",
    "- rf_fedrg_0 - No\n",
    "- rf_artec_0 - No\n",
    "- rf_cesar_0 - No\n",
    "- ip_gon_0 - No\n",
    "- ip_syph_0 - No\n",
    "- ip_chlam_0 - No\n",
    "- ip_hepb_0 - No\n",
    "- ip_hepc_0 - No\n",
    "- ld_indl_0 - No\n",
    "- ld_augm_0 - No\n",
    "- ld_ster_0 - No\n",
    "- ld_antb_0 - No\n",
    "- ld_chor_0 - No\n",
    "- ld_anes_0 - No\n",
    "- me_pres_1 - Cephalic\n",
    "- me_rout_1 - Spontaneous\n",
    "- me_trial_0 - No\n",
    "- mm_mtr_0 - No\n",
    "- mm_rupt_0 - No\n",
    "- mm_uhyst_0 - No\n",
    "- mm_aicu_0 - No\n",
    "- attend_1 - Doctor of Medicine (MD)\n",
    "- pay_2 - Private Insurance\n",
    "- dplural_1 - Single\n",
    "- ab_aven1_0 - No\n",
    "- ab_aven6_0 - No\n",
    "- ab_nicu_0 - No\n",
    "- ab_surf_0 - No\n",
    "- ab_anti_0 - No\n",
    "- ab_seiz_0 - No\n",
    "- ca_down_0 - No\n",
    "- ca_disor_0 - No\n",
    "- dob_mm_1 - January"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca0c567",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['bfacil_1', 'mracehisp_1', 'mar_p_1', 'meduc_1', 'fracehisp_1', 'feduc_1', 'cig_rec_0', 'rf_pdiab_0', \n",
    "                 'rf_gdiab_0', 'rf_phype_0', 'rf_ghype_0', 'rf_ehype_0', 'rf_ppterm_0', 'rf_fedrg_0', 'rf_artec_0',\n",
    "                 'rf_cesar_0', 'ip_gon_0', 'ip_syph_0', 'ip_chlam_0', 'ip_hepb_0', 'ip_hepc_0', 'ld_indl_0', 'ld_augm_0',\n",
    "                 'ld_ster_0', 'ld_antb_0', 'ld_chor_0', 'ld_anes_0', 'me_pres_1', 'me_rout_1', 'me_trial_0', 'mm_mtr_0',\n",
    "                 'mm_rupt_0', 'mm_uhyst_0', 'mm_aicu_0', 'pay_2', 'dplural_1', 'ab_aven1_0', 'ab_aven6_0',\n",
    "                 'ab_nicu_0', 'ab_surf_0', 'ab_anti_0', 'ab_seiz_0', 'dob_mm_1'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ffde01",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df = (\n",
    "    df.isna()\n",
    "      .sum()\n",
    "      .sort_values(ascending=False)\n",
    "      .reset_index()\n",
    "      .set_axis(['variable', 'missing'], axis=1)\n",
    ")\n",
    "missing_df['missing_pct'] = missing_df['missing'] / len(df)\n",
    "\n",
    "missing_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc8a7b7",
   "metadata": {},
   "source": [
    "Drop features that have a count of missing values above a specific threshold. Impute based on the median of the data points' given year of measurement otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8798cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.20\n",
    "\n",
    "df = df.drop(columns=missing_df[missing_df['missing_pct'] > threshold]['variable'], axis=1)\n",
    "\n",
    "for var in missing_df['variable']:\n",
    "    if var not in df.columns:\n",
    "        continue\n",
    "\n",
    "    missing_pct = missing_df.loc[missing_df['variable'] == var, 'missing_pct'].values[0]\n",
    "\n",
    "    if missing_pct > 0 and df[var].dtype in ['float64']:\n",
    "        df[var] = (\n",
    "            df.groupby(df['date'].dt.year)[var]\n",
    "              .transform(lambda x: x.fillna(x.median()))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b52c40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df = (\n",
    "    df.isna()\n",
    "      .sum()\n",
    "      .sort_values(ascending=False)\n",
    "      .reset_index()\n",
    "      .set_axis(['variable', 'missing'], axis=1)\n",
    ")\n",
    "missing_df['missing_pct'] = missing_df['missing'] / len(df)\n",
    "\n",
    "missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d07c482",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in continuous_cols:\n",
    "    nbins = min(50, df[col].nunique())\n",
    "    fig = px.histogram(\n",
    "        df,\n",
    "        x=col,\n",
    "        nbins=nbins,\n",
    "        color_discrete_sequence=[\"cornflowerblue\"],\n",
    "        title=f\"Distribution of {col}\",\n",
    "        labels={col: col, \"count\": \"Count\"}\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        bargap=0.05,\n",
    "        template=\"plotly_white\",\n",
    "        xaxis_title=col,\n",
    "        yaxis_title=\"Count\"\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74942225",
   "metadata": {},
   "source": [
    "Several columns are extremely skewed. Lets see if log1p transformation can reign in variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dba272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [\"cig_0\", \"cig_1\", \"cig_2\", \"cig_3\", 'priorlive', 'priordead', 'priorterm', 'precare']:\n",
    "  orig_var = df[c].var()\n",
    "  log_var = np.log1p(df[c]).var()\n",
    "  print(f\"Variance of {c}: {orig_var};  {log_var}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61614fda",
   "metadata": {},
   "source": [
    "Some columns are also extremely zero-inflated, so let's break them out into seprate binary and log1p transformed versions to encode both y/n and \"magnitude\". We can select / prune later on. Let's also skip the transform of priordead and just turn it into a binary - magnitude may not meaningfully add to the model since most have between 0-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7840e61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cig_0_binary\"] = (df[\"cig_0\"] > 0).astype(int)\n",
    "df[\"cig_0_log1p\"] = np.log1p(df[\"cig_0\"])\n",
    "\n",
    "df[\"cig_1_binary\"] = (df[\"cig_1\"] > 0).astype(int)\n",
    "df[\"cig_1_log1p\"] = np.log1p(df[\"cig_1\"])\n",
    "\n",
    "df[\"cig_2_binary\"] = (df[\"cig_2\"] > 0).astype(int)\n",
    "df[\"cig_2_log1p\"] = np.log1p(df[\"cig_2\"])\n",
    "\n",
    "df[\"cig_3_binary\"] = (df[\"cig_3\"] > 0).astype(int)\n",
    "df[\"cig_3_log1p\"] = np.log1p(df[\"cig_3\"])\n",
    "\n",
    "df[\"priorlive_binary\"] = (df[\"priorlive\"] > 0).astype(int)\n",
    "df[\"priorlive_log1p\"] = np.log1p(df[\"priorlive\"])\n",
    "\n",
    "df[\"priordead_binary\"] = (df[\"priordead\"] > 0).astype(int)\n",
    "\n",
    "df[\"priorterm_binary\"] = (df[\"priorterm\"] > 0).astype(int)\n",
    "df[\"priorterm_log1p\"] = np.log1p(df[\"priorterm\"])\n",
    "\n",
    "df[\"precare_binary\"] = (df[\"precare\"] > 0).astype(int)\n",
    "df[\"precare_log1p\"] = np.log1p(df[\"precare\"])\n",
    "\n",
    "to_remove = [\"cig_0\", \"cig_1\", \"cig_2\", \"cig_3\", \"priorlive\", \"priordead\", \"priorterm\", \"precare\"]\n",
    "continuous_cols = [col for col in continuous_cols if col not in to_remove]\n",
    "continuous_cols.extend([\"cig_0_log1p\", \"cig_1_log1p\", \"cig_2_log1p\", \"cig_3_log1p\", \"priorlive_log1p\", \"priorterm_log1p\", \"precare_log1p\"])\n",
    "binary_cols.extend([\"cig_0_binary\", \"cig_1_binary\", \"cig_2_binary\", \"cig_3_binary\", \"priorlive_binary\", \"priordead_binary\", \"priorterm_binary\", \"precare_binary\"])\n",
    "\n",
    "df = df.drop(columns=to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98468ede",
   "metadata": {},
   "source": [
    "<h2>Correlation with Pearson Coefficient (Linear Relationships)</h2>\n",
    "\n",
    "- Works well between:\n",
    "    - Two continuous variables.\n",
    "    - A binary and a continuous variable (interpretable as difference in means).\n",
    "    - Between two binaries, Pearson is equivalent to the phi coefficient, which is basically a normalized chi-square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048a9748",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols_all = [\n",
    "    col\n",
    "    for col in df.columns\n",
    "    if any(substring in col for substring in cat_cols)\n",
    "    and re.search(r'_\\d+$', col)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0796d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df[continuous_cols + binary_cols + cat_cols_all + ['death_reported']].corr(numeric_only=True)\n",
    "\n",
    "# lower triangle\n",
    "tri_lower_mask = np.tril(np.ones_like(corr_matrix, dtype=bool), k=-1)\n",
    "corr_long = (\n",
    "    corr_matrix.where(tri_lower_mask)\n",
    "            .stack() # to long form\n",
    "            .reset_index()\n",
    "            .rename(columns={'level_0':'variable_1', 'level_1':'variable_2', 0:'corr'})\n",
    ")\n",
    "\n",
    "corr_long['abs_corr'] = corr_long['corr'].abs()\n",
    "top_corr = corr_long.sort_values('abs_corr', ascending=False).reset_index(drop=True)\n",
    "\n",
    "var_to_name_map = json.load(open(BASE_DIR / \"var_name_map.json\"))\n",
    "\n",
    "top_corr['variable_1_full'] = top_corr['variable_1'].map(var_to_name_map)\n",
    "top_corr['variable_2_full'] = top_corr['variable_2'].map(var_to_name_map)\n",
    "\n",
    "top_corr = top_corr[['variable_1', 'variable_2', 'variable_1_full', 'variable_2_full', 'corr', 'abs_corr']]\n",
    "\n",
    "top_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f440cedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_corr_with_outcome = top_corr[(top_corr['variable_2'] == 'death_reported') | (top_corr['variable_1'] == 'death_reported')]\n",
    "\n",
    "top_corr_with_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135ff579",
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_upper_mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "corr_masked = corr_matrix.mask(tri_upper_mask)\n",
    "\n",
    "fig = px.imshow(\n",
    "    corr_masked,\n",
    "    text_auto=\".2f\",\n",
    "    color_continuous_scale=\"RdBu_r\",\n",
    "    zmin=-1, zmax=1,\n",
    "    title=\"Linear Correlations (Continuous + One-Hot Variables)\",\n",
    "    aspect=\"auto\"\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    template=\"plotly_white\",\n",
    "    coloraxis_colorbar=dict(title=\"Pearson r\"),\n",
    "    width=1500,\n",
    "    height=900\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c25bba",
   "metadata": {},
   "source": [
    "<h2>Clustering Analysis</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dc941b",
   "metadata": {},
   "source": [
    "Factor Analysis of Mixed Data (FAMD) - Dimensionality reduction technique designed specifically for datasets that contain both categorical and continuous variables. For linear associations.\n",
    "\n",
    "Sort of a hybrid of:\n",
    "\n",
    "- PCA for continuous variables.\n",
    "- MCA (Multiple Correspondence Analysis) for categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb733c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not using the expanding scaler here, as this is not a supervised task - we're not trying to predict the outcome.\n",
    "# Fine if the entirety of the data is used at once for mean and std.\n",
    "X_num = StandardScaler().fit_transform(df[continuous_cols])\n",
    "\n",
    "pca = PCA(n_components=2).fit_transform(X_num)\n",
    "mca = TruncatedSVD(n_components=2).fit_transform(df[binary_cols + cat_cols_all])\n",
    "\n",
    "pca_3d = PCA(n_components=3).fit_transform(X_num)\n",
    "mca_3d = TruncatedSVD(n_components=3).fit_transform(df[binary_cols + cat_cols_all])\n",
    "\n",
    "X_famd_like = np.hstack([pca, mca])\n",
    "X_famd_like_3d = np.hstack([pca_3d, mca_3d])\n",
    "\n",
    "# Too compute heavy - stalled even on my local.\n",
    "#agg = AgglomerativeClustering(n_clusters=4, linkage='ward')\n",
    "# labels_agg = agg.fit_predict(X_famd_like)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "labels_km = kmeans.fit_predict(X_famd_like)\n",
    "\n",
    "kmeans_3d = KMeans(n_clusters=3, random_state=42)\n",
    "labels_km_3d = kmeans.fit_predict(X_famd_like_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f74ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_str = labels_km.astype(str)\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=X_famd_like[:, 0],\n",
    "    y=X_famd_like[:, 1],\n",
    "    color=cluster_str,\n",
    "    labels={\n",
    "        'x': 'Component 1',\n",
    "        'y': 'Component 2',\n",
    "        'color': 'Cluster'\n",
    "    },\n",
    "    title='FAMD-like Embedding: PCA + SVD',\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    template=\"plotly_white\",\n",
    "    width=900,\n",
    "    height=700\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae3dc3b",
   "metadata": {},
   "source": [
    "NOTE: 3D charts are saved via .html files and code blocks are commented out - they use a lot of RAM since they're interactive, and cause freezing up if rendered in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540e325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_plot = pd.DataFrame(X_famd_like[:, :3], columns=['Comp1', 'Comp2', 'Comp3'])\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    df_plot,\n",
    "    x='Comp1', y='Comp2', z='Comp3',\n",
    "    color=cluster_str,\n",
    "    color_continuous_scale='Viridis',\n",
    "    opacity=0.8,\n",
    "    title='3D FAMD-esque Embedding',\n",
    "    labels={\n",
    "        'color': 'Cluster'\n",
    "    },\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=dict(size=5))\n",
    "# fig.write_html(\"famd_embedding_cluster_labeled.html\")\n",
    "fig.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566c9a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_y_series = df['death_reported'].replace({0: 'No', 1: 'Yes'})\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=X_famd_like[:, 0],\n",
    "    y=X_famd_like[:, 1],\n",
    "    color=scatter_y_series,\n",
    "    labels={\n",
    "        'x': 'Component 1',\n",
    "        'y': 'Component 2',\n",
    "        'color': 'Death Reported'\n",
    "    },\n",
    "    title='FAMD-like Embedding: PCA + SVD',\n",
    "    color_discrete_map={'No': 'cornflowerblue', 'Yes': 'tomato'}\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    template=\"plotly_white\",\n",
    "    width=900,\n",
    "    height=700\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a809a069",
   "metadata": {},
   "source": [
    "So there turns out to be something interesting here when we scrap the clustering algorithm and use the 'Death Reported' outcome variable as the colormap. We see a grouping of the 'Yeses', visible in both the 2D and 3D scatterplots. Though we aren't getting much new information here (this is structure we know about), what seems apparent is that there is the opportunity for a sort of rough decision-boundary, which means there may be enough linear signal here to roll with Logistic Regression. If they were scattered throughout and perfectly intertwined, I would say maybe lets try UMAP/HDBSCAN to identify non-linear structure, or default to a non-linear ML model for the classification task. But it looks like a solid candidtae for Logistic Regression right now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961a32b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "fig = px.scatter_3d(\n",
    "    df_plot,\n",
    "    x='Comp1', y='Comp2', z='Comp3',\n",
    "    color=scatter_y_series,\n",
    "    color_continuous_scale='Viridis',\n",
    "    opacity=0.8,\n",
    "    title='3D FAMD-esque Embedding',\n",
    "    labels={\n",
    "        'color': 'Death Reported'\n",
    "    },\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=dict(size=5))\n",
    "# fig.write_html(\"famd_embedding_outcome_labeled.html\")\n",
    "fig.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c3078c",
   "metadata": {},
   "source": [
    "<h1>Feature Engineering & Selection</h1>\n",
    "\n",
    "I was going to include these in EDA, however I think to keep things clean, it might make more sense to break these out into a separate notebook, which I will work on next.\n",
    "\n",
    "LinearFeatureSelector was kept here and I dropped in our expanding scaler as well. The Selector was adapted slightly from its original use case to work with this project and the expanded scaler. Will answer any questions about this, since I wrote them myself :) But high-level, the goal is to generate a diverse set of possible feature sets, with some that generated better R^2 values over F1 and Condition Number, some with better F1 than the other two metrics, and some with a better Condition Number than the others.\n",
    "\n",
    "So this does not output a single ideal, because in this optimization problem (Multi-Objective or 'MOO'), there isn't one.\n",
    "\n",
    "Why MOO?\n",
    "\n",
    "This is a causal model, so classification performance (F1) is not everything. It should do well, but that not enough. We also want goodness-of-fit (R^2) and we want to avoid too much multicollinearity (Kappa / Condition Number). So this gives us combinations of features to explore with different performance skews. We can combine these options with what domain expertise and our learned DAG tell us to make better association / quasi-causal statments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f250c410",
   "metadata": {},
   "source": [
    "<h2>Build-a-Dag Workshop</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906d1010",
   "metadata": {},
   "source": [
    "<h2>Interaction Exploration</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1362399e",
   "metadata": {},
   "source": [
    "<h2>Feature Selection</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357d2f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_expanding_standard_scaler_by_date(df, date_col, merge_cols=None, min_periods=1):\n",
    "    \"\"\"\n",
    "    Expanding z-score scaling by unique date (allows multiple data points per date).\n",
    "    Prevents leakage by using only past data.\n",
    "\n",
    "    merge_cols is a list of columns that are not scaled. They're called this because\n",
    "    they're merged back to the columns with the date_col.\n",
    "    \"\"\"\n",
    "    if merge_cols is None:\n",
    "        merge_cols = []\n",
    "\n",
    "    df = df.sort_values(date_col)\n",
    "\n",
    "    feature_cols = [col for col in df.columns if col !=\n",
    "                    date_col and col not in merge_cols]\n",
    "\n",
    "    means_map = {}\n",
    "    stds_map = {}\n",
    "\n",
    "    unique_dates = df[date_col].drop_duplicates().sort_values().to_numpy()\n",
    "    for i, current_date in enumerate(unique_dates):\n",
    "        if i < min_periods:\n",
    "            continue\n",
    "\n",
    "        past_data = df[df[date_col] <= current_date][feature_cols].to_numpy()\n",
    "        if past_data.shape[0] == 0:\n",
    "            continue\n",
    "        # If there is only one feature column, must be reshapped to return compatible shape.\n",
    "        if past_data.ndim == 1:\n",
    "            if len(feature_cols) == 1:\n",
    "                past_data = past_data.reshape(-1, 1)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Expected {len(feature_cols)} features but got 1D array on {current_date}\")\n",
    "\n",
    "        if past_data.shape[1] != len(feature_cols):\n",
    "            raise ValueError(\n",
    "                f\"Feature count mismatch at {current_date}: expected {len(feature_cols)} and got {past_data.shape[1]}. Check for duplicates.\")\n",
    "\n",
    "        means = np.mean(past_data, axis=0)\n",
    "        stds = np.std(past_data, axis=0, ddof=0)\n",
    "        means_map[current_date] = means\n",
    "        stds_map[current_date] = np.where(stds == 0, 1e-8, stds)\n",
    "\n",
    "    scaled_array = np.full((len(df), len(feature_cols)), np.nan)\n",
    "\n",
    "    date_to_index = {d: np.where(df[date_col] == d)[0] for d in unique_dates}\n",
    "\n",
    "    for i, current_date in enumerate(unique_dates):\n",
    "        if current_date not in means_map:\n",
    "            continue\n",
    "        idx = date_to_index[current_date]\n",
    "        X = df.iloc[idx][feature_cols].to_numpy()\n",
    "        scaled = (X - means_map[current_date]) / stds_map[current_date]\n",
    "        scaled_array[idx, :] = scaled\n",
    "\n",
    "    scaled_df = pd.DataFrame(scaled_array, columns=feature_cols)\n",
    "    scaled_df[[date_col] + merge_cols] = df[[date_col] + merge_cols].values\n",
    "    scaled_df = scaled_df.dropna(subset=feature_cols, how=\"all\", axis=0)\n",
    "\n",
    "    return scaled_df\n",
    "\n",
    "class LinearFeatureSelector:\n",
    "    def __init__(self, n_trials=200, random_state=42):\n",
    "        self.n_trials = n_trials\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def _eval_subset(self, X, y, binary_vars, date_col, cols):\n",
    "        # if empty subset, return terrible scores so NSGA-II moves away from it\n",
    "        if len(cols) == 0:\n",
    "            return -1e6, 1e6, 1e6\n",
    "        \n",
    "        if binary_vars is None:\n",
    "            binary_vars = []\n",
    "\n",
    "        X_sub = X[cols]\n",
    "\n",
    "        cv = TimeSeriesSplit(n_splits=5, test_size=4, gap=0)\n",
    "\n",
    "        f1s = []\n",
    "        r2s = []\n",
    "        for train_index, test_index in cv.split(X_sub):\n",
    "            X_train, X_test = X_sub.iloc[train_index], X_sub.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "            X_train_needs_scaling = X_train[[col for col in X_train.columns if col not in binary_vars]]\n",
    "            X_test_needs_scaling = X_test[[col for col in X_test.columns if col not in binary_vars]]\n",
    "            \n",
    "            X_train_scaled = global_expanding_standard_scaler_by_date(X_train_needs_scaling, date_col, min_periods=1)\n",
    "            X_test_scaled = global_expanding_standard_scaler_by_date(X_test_needs_scaling, date_col, min_periods=1)\n",
    "\n",
    "            X_train_scaled_with_unscaled = pd.concat([X_train_scaled, X_train[[col for col in X_train.columns if col in binary_vars]]], axis=1)\n",
    "            X_test_scaled_with_unscaled = pd.concat([X_test_scaled, X_test[[col for col in X_test.columns if col in binary_vars]]], axis=1)\n",
    "\n",
    "            lr = LogisticRegression(n_jobs=-1, random_state=self.random_state).fit(X_train_scaled_with_unscaled, y_train)\n",
    "            \n",
    "            yhat = lr.predict(X_test_scaled_with_unscaled)\n",
    "            \n",
    "            in_sample_r2 = r2_score(y_test, yhat)\n",
    "            oos_f1 = f1_score(y_test, yhat)\n",
    "            r2s.append(in_sample_r2)\n",
    "            f1s.append(oos_f1)\n",
    "\n",
    "        avg_r2 = float(np.mean(r2s))\n",
    "        avg_f1 = float(np.mean(f1s))\n",
    "\n",
    "        X_group_needs_scaling = X_sub[[col for col in X_sub.columns if col not in binary_vars]]\n",
    "        X_group_scaled = global_expanding_standard_scaler_by_date(X_group_needs_scaling, \"date\", [\"date\"], min_periods=1)\n",
    "        X_group_scaled_with_unscaled = pd.concat([X_group_scaled, X_sub[[col for col in X_sub.columns if col in binary_vars]]], axis=1)\n",
    "        condition_number_kappa = np.linalg.cond(sm.add_constant(X_group_scaled_with_unscaled))\n",
    "\n",
    "        return avg_r2, avg_f1, condition_number_kappa\n",
    "\n",
    "\n",
    "    def _run_nsga_feature_search(self, X, y, binary_vars, established_model_variables, date_col):\n",
    "\n",
    "        df = pd.concat([X, y], axis=1).dropna(axis=1, how=\"all\").dropna()\n",
    "        X = df[X.columns.intersection(df.columns)]\n",
    "        y = df[y.name] if getattr(y, \"name\", None) else df.iloc[:, -1]\n",
    "\n",
    "        established_model_variables = established_model_variables or []\n",
    "        for c in established_model_variables:\n",
    "            if c not in X.columns:\n",
    "                raise ValueError(f\"Control variable '{c}' not in X.\")\n",
    "\n",
    "        candidate_cols = [c for c in X.columns if c not in established_model_variables]\n",
    "\n",
    "        def __objective(trial: optuna.Trial):\n",
    "            chosen_cols = []\n",
    "            for col in candidate_cols:\n",
    "                use_col = trial.suggest_categorical(f\"use::{col}\", [0, 1])\n",
    "                if use_col == 1:\n",
    "                    chosen_cols.append(col)\n",
    "\n",
    "            eval_set = established_model_variables + chosen_cols\n",
    "\n",
    "            r2, f1, kappa = self._eval_subset(X, y, binary_vars, date_col, eval_set)\n",
    "\n",
    "            trial.set_user_attr(\"cols\", chosen_cols)\n",
    "\n",
    "            return r2, f1, kappa\n",
    "\n",
    "        study = optuna.create_study(\n",
    "            directions=[\"maximize\", \"maximize\", \"minimize\"], # R2 higher better, F1 higher better, kappa (condiition number) lower better\n",
    "            sampler=optuna.samplers.NSGAIISampler(seed=self.random_state),\n",
    "        )\n",
    "        study.optimize(__objective, n_trials=self.n_trials)\n",
    "\n",
    "        return study\n",
    "    \n",
    "    def _build_pareto_front_table(self, study):\n",
    "        rows = []\n",
    "        # Each trial contains a candidate set of features; we don't scrap any.\n",
    "        # Result is a Pareto Front, with the top being the non-dominated ones.\n",
    "        for tr in study.best_trials:\n",
    "            r2, f1, kappa = tr.values\n",
    "            cols = tr.user_attrs.get(\"cols\", [])\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"trial_id\": tr.number,\n",
    "                    \"r2\": r2,\n",
    "                    \"f1\": f1,\n",
    "                    \"kappa\": kappa,\n",
    "                    \"n_features\": len(cols),\n",
    "                    \"features\": cols,\n",
    "                }\n",
    "            )\n",
    "        df_pf = pd.DataFrame(rows).sort_values(\n",
    "            [\"r2\", \"f1\", \"kappa\"], ascending=[False, False, True]\n",
    "        )\n",
    "        return df_pf.reset_index(drop=True)\n",
    "    \n",
    "    def fit(self, X, y, date_col, binary_vars=None, established_model_variables=None):\n",
    "        study = self._run_nsga_feature_search(X, y, binary_vars, established_model_variables, date_col)\n",
    "        pareto_table = self._build_pareto_front_table(study)\n",
    "        return pareto_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
