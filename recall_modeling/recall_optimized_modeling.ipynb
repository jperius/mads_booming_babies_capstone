{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ad873ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    log_loss,\n",
    "    roc_auc_score\n",
    "\n",
    ")\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "from capstone.expanding_scaler import global_expanding_standard_scaler_by_date\n",
    "\n",
    "BASE_DIR = Path().resolve().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c54d442",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizerClassifier():\n",
    "    def __init__(\n",
    "            self,\n",
    "            search_iter=5000,\n",
    "            decision_threshold=0.5,\n",
    "            scoring_metric='recall',\n",
    "            xgb_objective='binary:logistic',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "    ):\n",
    "        self.search_iter = search_iter\n",
    "        self.decision_threshold = decision_threshold\n",
    "        self.scoring_metric = scoring_metric\n",
    "        self.xgb_objective = xgb_objective\n",
    "        self.random_state = random_state\n",
    "        self.n_jobs = n_jobs\n",
    "        self.base_params = {\n",
    "            \"random_state\": self.random_state,\n",
    "            \"n_jobs\": self.n_jobs,\n",
    "        }\n",
    "        self.scorer = self._make_scorer_cust(self.scoring_metric)\n",
    "        self.binary_vars = None\n",
    "        self.date_col = None\n",
    "        self.cv = None\n",
    "        self.scaled_data = {}\n",
    "        self.best_estimator = None\n",
    "        self.best_score = None\n",
    "        self.best_params = None\n",
    "        self.best_use_balance = None\n",
    "\n",
    "    def _make_scorer_cust(self, scoring_metric: str):\n",
    "        if scoring_metric in ['logloss', 'mlogloss']:\n",
    "            return log_loss\n",
    "        # Uses 1 - accuracy to align with XGBoost error.\n",
    "        elif scoring_metric in ['error', 'merror']:\n",
    "            return lambda y_true, y_pred: 1 - (accuracy_score(y_true, y_pred))\n",
    "        elif scoring_metric == \"recall\":\n",
    "            return recall_score\n",
    "        elif scoring_metric == \"precision\":\n",
    "            return precision_score\n",
    "        elif scoring_metric == \"auc\":\n",
    "            return roc_auc_score\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported scoring metric: {scoring_metric}\")\n",
    "    \n",
    "    def _get_scaled_train_test_groups(self, X, y):\n",
    "\n",
    "        order_idx = X[self.date_col].sort_values().index\n",
    "        X_sorted = X.loc[order_idx]\n",
    "\n",
    "        cont_cols = [c for c in X.columns if c not in self.binary_vars and c != self.date_col]\n",
    "                \n",
    "        if self.cv is None:\n",
    "            \n",
    "            tscv = TimeSeriesSplit(\n",
    "                n_splits=3,\n",
    "                test_size=int(round(X_sorted.shape[0] * 0.10, 0)),\n",
    "                gap=0,\n",
    "            )\n",
    "            self.cv = list(tscv.split(X_sorted))\n",
    "\n",
    "            for split, (train_index, test_index) in enumerate(self.cv):\n",
    "                X_train, X_test = X_sorted.iloc[train_index], X_sorted.iloc[test_index]\n",
    "\n",
    "                X_train_scaled = X_train.copy()\n",
    "                X_test_scaled  = X_test.copy()\n",
    "\n",
    "                X_train_scaled[cont_cols] = X_train_scaled[cont_cols].astype(float)\n",
    "                X_test_scaled[cont_cols]  = X_test_scaled[cont_cols].astype(float)\n",
    "\n",
    "                train_for_scaler = X_train_scaled[cont_cols + [self.date_col]]\n",
    "                train_scaled_full, train_scaler_state = global_expanding_standard_scaler_by_date(\n",
    "                    train_for_scaler,\n",
    "                    date_col=self.date_col,\n",
    "                    merge_cols=[self.date_col],\n",
    "                    min_periods=0,\n",
    "                    return_stats=True,\n",
    "                )\n",
    "                X_train_scaled.loc[train_scaled_full.index, cont_cols] = train_scaled_full[cont_cols]\n",
    "\n",
    "                test_for_scaler = X_test_scaled[cont_cols + [self.date_col]]\n",
    "                test_scaled_full = global_expanding_standard_scaler_by_date(\n",
    "                    test_for_scaler,\n",
    "                    date_col=self.date_col,\n",
    "                    merge_cols=[self.date_col],\n",
    "                    min_periods=0,\n",
    "                    stats=train_scaler_state,\n",
    "                    return_stats=False,\n",
    "                )\n",
    "                X_test_scaled.loc[test_scaled_full.index, cont_cols] = test_scaled_full[cont_cols]\n",
    "\n",
    "                X_train_lr = X_train_scaled.drop(columns=[self.date_col])\n",
    "                X_test_lr = X_test_scaled.drop(columns=[self.date_col])\n",
    "\n",
    "                self.scaled_data[f'train_{split}'] = X_train_lr\n",
    "                self.scaled_data[f'test_{split}'] = X_test_lr\n",
    "\n",
    "            X_group_needs_scaling = X[cont_cols + [self.date_col]]\n",
    "            X_group_scaled = global_expanding_standard_scaler_by_date(\n",
    "                X_group_needs_scaling,\n",
    "                date_col=self.date_col,\n",
    "                merge_cols=[self.date_col],\n",
    "                min_periods=0\n",
    "            )\n",
    "            X_group_scaled_no_date = X_group_scaled.drop(columns=[self.date_col])\n",
    "            self.scaled_data['all'] = X_group_scaled_no_date\n",
    "\n",
    "    def _eval_classifier(self, X, y, model_params):\n",
    "\n",
    "        if self.cv is None:\n",
    "            raise ValueError('self.cv is not set.')\n",
    "        \n",
    "        order_idx = X[self.date_col].sort_values().index\n",
    "        y_sorted = y.loc[order_idx]\n",
    "        \n",
    "        fold_scores = []\n",
    "        for split, (train_index, test_index) in enumerate(self.cv):\n",
    "            X_train = self.scaled_data[f'train_{split}']\n",
    "            X_test  = self.scaled_data[f'test_{split}']\n",
    "\n",
    "            y_train, y_test = y_sorted.iloc[train_index], y_sorted.iloc[test_index]\n",
    "            \n",
    "            # We aren't using the pruining callback because it would interrupt the\n",
    "            # k-fold cross-validation. Instead, we use early stopping as a parameter\n",
    "            # of the model, and allow Optuna to then decide where to search next.\n",
    "            model = self.ModelClass(**model_params)\n",
    "\n",
    "            if self.ModelClass is XGBClassifier:\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=[(X_test, y_test)],\n",
    "                    verbose=False\n",
    "                )\n",
    "            else:\n",
    "                model.fit(X_train,y_train)\n",
    "            # During training with an eval_set and early_stopping_rounds,\n",
    "            # XGBoost tracks the validation score at each boosting round.\n",
    "            # When validation stops improving for early_stopping_rounds\n",
    "            # consecutive rounds, training halts and best_iteration is then\n",
    "            # set to the boosting round (0-based index) with the best validation score.\n",
    "            best_iter = getattr(model, \"best_iteration\", None)\n",
    "            use_proba = self.scoring_metric in (\"logloss\", \"mlogloss\", \"auc\")\n",
    "\n",
    "            if best_iter is not None:\n",
    "                # y_proba_train = model.predict_proba(X_train, iteration_range=(0, best_iter + 1))[:, 1]\n",
    "                y_proba_test = model.predict_proba(X_test, iteration_range=(0, best_iter + 1))[:, 1]\n",
    "            else:\n",
    "                # y_proba_train = model.predict_proba(X_train)[:, 1]\n",
    "                y_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            # yhat_train = (y_proba_train >= decision_threshold).astype(int)\n",
    "            yhat_test = (y_proba_test  >= self.decision_threshold).astype(int)\n",
    "\n",
    "            if use_proba:\n",
    "                fold_score = self.scorer(y_test, y_proba_test)\n",
    "            else:\n",
    "                fold_score = self.scorer(y_test, yhat_test)\n",
    "\n",
    "            fold_scores.append(fold_score)\n",
    "        \n",
    "        return float(np.mean(fold_scores))\n",
    "\n",
    "    def _run_optimization(self, X, y):\n",
    "\n",
    "        self._get_scaled_train_test_groups(X, y)\n",
    "\n",
    "        def __objective(trial: optuna.Trial) -> float:\n",
    "\n",
    "            if self.cv is None or not isinstance(self.cv, list):\n",
    "                raise ValueError('cv_splits is not set.')\n",
    "            if X is None:\n",
    "                raise ValueError('X is not set.')\n",
    "            if y is None:\n",
    "                raise ValueError('y is not set.')\n",
    "            if self.ModelClass is None:\n",
    "                raise ValueError('ModelClass is not set.')\n",
    "            \n",
    "            if self.ModelClass is XGBClassifier:\n",
    "                neg = (y == 0).sum()\n",
    "                pos = (y == 1).sum()\n",
    "                balance_eq = neg / pos\n",
    "\n",
    "                use_balance = trial.suggest_categorical(\"use_balance_weight\", [True, False])\n",
    "                model_params = {\n",
    "                    **self.base_params,\n",
    "                    \"n_estimators\": 3000,\n",
    "                    \"early_stopping_rounds\": 50,\n",
    "                    \"objective\": self.xgb_objective,\n",
    "                    \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
    "                    \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "                    \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 0.5, 20.0, log=True),\n",
    "                    \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "                    \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "                    \"gamma\": trial.suggest_float(\"gamma\", 1e-9, 10.0, log=True),\n",
    "                }\n",
    "                if use_balance:\n",
    "                    model_params[\"scale_pos_weight\"] = balance_eq\n",
    "            else:\n",
    "                model_params = {\n",
    "                    **self.base_params,\n",
    "                    \"max_iter\": 1000,\n",
    "                    # \"solver\": \"liblinear\",\n",
    "                    \"class_weight\": trial.suggest_categorical(\"class_weight\", [\"balanced\", None]),\n",
    "                    \"C\": trial.suggest_float(\"C\", 1e-9, 10.0, log=True),\n",
    "                    #\"penalty\": trial.suggest_categorical(\"penalty\", [\"l1\", \"l2\"]),\n",
    "                }\n",
    "            \n",
    "\n",
    "            score = self._eval_classifier(X, y, model_params)\n",
    "\n",
    "            return score\n",
    "        \n",
    "        if self.scoring_metric in (\"logloss\", \"mlogloss\", \"error\", \"merror\"):\n",
    "            direction = \"minimize\"\n",
    "        else:\n",
    "            direction = \"maximize\"\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction=direction,\n",
    "            pruner=MedianPruner(n_min_trials=self.search_iter // 2),\n",
    "            sampler=TPESampler(seed=self.random_state)\n",
    "        )\n",
    "        study.optimize(\n",
    "                __objective,\n",
    "            n_trials=self.search_iter\n",
    "        )\n",
    "\n",
    "        order_idx = X[self.date_col].sort_values().index\n",
    "        X_all_scaled = self.scaled_data['all'].loc[order_idx]\n",
    "        y_sorted = y.loc[order_idx]\n",
    "\n",
    "        study_params = dict(study.best_params)\n",
    "\n",
    "        use_balance = study_params.pop(\"use_balance_weight\", None)\n",
    "        best_model_params = {**self.base_params, **study_params}\n",
    "        best_estimator = self.ModelClass(**best_model_params)\n",
    "        best_estimator.fit(X_all_scaled, y_sorted)\n",
    "\n",
    "        self.best_estimator = best_estimator\n",
    "        self.best_score = study.best_value\n",
    "        self.best_params = best_model_params\n",
    "        self.best_use_balance = use_balance\n",
    "\n",
    "    def fit_transform(\n",
    "        self,\n",
    "        X,\n",
    "        y,\n",
    "        date_col,\n",
    "        binary_vars=None,\n",
    "        model_type='xgb_clf'\n",
    "    ):\n",
    "        \n",
    "        self.date_col = date_col\n",
    "        self.binary_vars = binary_vars or []\n",
    "        \n",
    "        if model_type == 'xgb_clf':\n",
    "            self.ModelClass = XGBClassifier\n",
    "        elif model_type == 'lr':\n",
    "            self.ModelClass = LogisticRegression\n",
    "        else:\n",
    "            raise ValueError(f\"Model type {model_type} is not supported.\")\n",
    "\n",
    "        self._run_optimization(X, y)\n",
    "\n",
    "        return (self.best_estimator, self.best_score, self.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ab690a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(BASE_DIR / 'recall_modeling' / 'natality_10yr_test_data_cat.csv')\n",
    "\n",
    "X = df.drop(columns=['morbidity_reported'])\n",
    "y = df['morbidity_reported']\n",
    "\n",
    "binary_vars = [\n",
    "    'dmar',\n",
    "    'ca_anen',\n",
    "    'ca_mnsb',\n",
    "    'ca_cchd',\n",
    "    'ca_cdh',\n",
    "    'ca_omph',\n",
    "    'ca_gast',\n",
    "    'ca_limb',\n",
    "    'ca_cleft',\n",
    "    'ca_clpal',\n",
    "    'ca_hypo',\n",
    "    'sex',\n",
    "    'ca_down',\n",
    "    'precare_binary',\n",
    "    'prior_dead_term_binary',\n",
    "    'ca_disor',\n",
    "    'smoking',\n",
    "    'hospital_birth_binary'\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139ff31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = OptimizerClassifier(\n",
    "    search_iter=500,\n",
    "    random_state=42,\n",
    "    decision_threshold=0.5,\n",
    "    scoring_metric='recall',\n",
    "    xgb_objective='binary:logistic',\n",
    ")\n",
    "\n",
    "best_model, best_score, best_params = optimizer.fit_transform(\n",
    "    X,\n",
    "    y,\n",
    "    date_col='date',\n",
    "    binary_vars=binary_vars,\n",
    "    model_type='xgb_clf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac6e032",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer2 = OptimizerClassifier(\n",
    "    search_iter=500,\n",
    "    random_state=42,\n",
    "    decision_threshold=0.3,\n",
    "    scoring_metric='recall',\n",
    "    xgb_objective='binary:logistic',\n",
    ")\n",
    "\n",
    "best_model2, best_score2, best_params2 = optimizer2.fit_transform(\n",
    "    X,\n",
    "    y,\n",
    "    date_col='date',\n",
    "    binary_vars=binary_vars,\n",
    "    model_type='xgb_clf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6869a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(BASE_DIR / 'recall_modeling' / 'natality_10yr_test_data_one_hot.csv')\n",
    "\n",
    "X2 = df2.drop(columns=['morbidity_reported'])\n",
    "y2 = df2['morbidity_reported']\n",
    "\n",
    "binary_vars = [\n",
    " 'ca_down',\n",
    " 'precare_binary',\n",
    " 'prior_dead_term_binary',\n",
    " 'ca_disor',\n",
    " 'rf_inftr',\n",
    " 'rf_cesar',\n",
    " 'hospital_birth_binary',\n",
    " 'mracehisp_2',\n",
    " 'mracehisp_3',\n",
    " 'mracehisp_4',\n",
    " 'mracehisp_5',\n",
    " 'mracehisp_6',\n",
    " 'mracehisp_7',\n",
    " 'mracehisp_8',\n",
    " 'meduc_2',\n",
    " 'meduc_3',\n",
    " 'meduc_4',\n",
    " 'meduc_5',\n",
    " 'meduc_6',\n",
    " 'meduc_7',\n",
    " 'meduc_8',\n",
    " 'fracehisp_2',\n",
    " 'fracehisp_3',\n",
    " 'fracehisp_4',\n",
    " 'fracehisp_5',\n",
    " 'fracehisp_6',\n",
    " 'fracehisp_7',\n",
    " 'fracehisp_8',\n",
    " 'feduc_2',\n",
    " 'feduc_3',\n",
    " 'feduc_4',\n",
    " 'feduc_5',\n",
    " 'feduc_6',\n",
    " 'feduc_7',\n",
    " 'feduc_8',\n",
    " 'rf_pdiab_1',\n",
    " 'rf_gdiab_1',\n",
    " 'rf_phype_1',\n",
    " 'rf_ghype_1',\n",
    " 'cig_rec_0',\n",
    " 'cig_rec_1',\n",
    " 'rf_ehype_1',\n",
    " 'rf_ppterm_1',\n",
    " 'ip_gon_1',\n",
    " 'ip_syph_1',\n",
    " 'ip_chlam_1',\n",
    " 'ip_hepb_1',\n",
    " 'ip_hepc_1',\n",
    " 'ld_indl_1',\n",
    " 'ld_augm_1',\n",
    " 'ld_ster_1',\n",
    " 'ld_antb_1',\n",
    " 'ld_chor_1',\n",
    " 'ld_anes_1',\n",
    " 'me_pres_2',\n",
    " 'me_pres_3',\n",
    " 'me_rout_1',\n",
    " 'me_rout_2',\n",
    " 'me_rout_3',\n",
    " 'me_rout_4',\n",
    " 'attend_1',\n",
    " 'attend_2',\n",
    " 'attend_3',\n",
    " 'attend_4',\n",
    " 'attend_5',\n",
    " 'pay_1',\n",
    " 'pay_3',\n",
    " 'pay_4',\n",
    " 'pay_5',\n",
    " 'pay_6',\n",
    " 'pay_8',\n",
    " 'dplural_2',\n",
    " 'dplural_3',\n",
    " 'dplural_4',\n",
    " 'dplural_5',\n",
    " 'ab_aven1_1',\n",
    " 'ab_aven6_1',\n",
    " 'ab_nicu_1',\n",
    " 'ab_surf_1',\n",
    " 'ab_anti_1',\n",
    " 'ab_seiz_1',\n",
    " 'dob_mm_10',\n",
    " 'dob_mm_11',\n",
    " 'dob_mm_12',\n",
    " 'dob_mm_2',\n",
    " 'dob_mm_3',\n",
    " 'dob_mm_4',\n",
    " 'dob_mm_5',\n",
    " 'dob_mm_6',\n",
    " 'dob_mm_7',\n",
    " 'dob_mm_8'\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d588edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer3 = OptimizerClassifier(\n",
    "    search_iter=500,\n",
    "    random_state=42,\n",
    "    decision_threshold=0.5,\n",
    "    scoring_metric='recall',\n",
    "    xgb_objective='binary:logistic',\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "best_model3, best_score3, best_params3 = optimizer3.fit_transform(\n",
    "    X2,\n",
    "    y2,\n",
    "    date_col='date',\n",
    "    binary_vars=binary_vars,\n",
    "    model_type='lr'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a3dd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer4 = OptimizerClassifier(\n",
    "    search_iter=500,\n",
    "    random_state=42,\n",
    "    decision_threshold=0.3,\n",
    "    scoring_metric='recall',\n",
    "    xgb_objective='binary:logistic',\n",
    ")\n",
    "\n",
    "best_model4, best_score4, best_params4 = optimizer4.fit_transform(\n",
    "    X,\n",
    "    y,\n",
    "    date_col='date',\n",
    "    binary_vars=binary_vars,\n",
    "    model_type='lr'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
